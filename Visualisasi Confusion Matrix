import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, classification_report

# Confusion matrix from the image (input manually)
confusion_matrix = np.array([
    [348, 6, 8, 7, 6, 5, 10, 9, 9, 2, 7, 11, 6, 4, 6, 1, 8, 5, 3, 3, 3, 3, 2, 1],
    [9, 304, 2, 0, 2, 2, 1, 0, 6, 2, 1, 3, 4, 4, 6, 2, 3, 2, 2, 3, 1, 2, 4, 1],
    [11, 1, 391, 4, 2, 0, 0, 1, 6, 3, 0, 1, 6, 1, 1, 2, 1, 2, 4, 1, 2, 2, 1, 1],
    [15, 4, 5, 387, 3, 4, 3, 2, 3, 4, 3, 3, 3, 2, 4, 1, 3, 2, 3, 5, 3, 1, 2, 1],
    [6, 1, 2, 1, 251, 5, 5, 1, 7, 2, 1, 1, 4, 2, 4, 2, 3, 3, 2, 1, 2, 2, 1, 1],
    [8, 4, 3, 3, 7, 303, 6, 1, 5, 5, 3, 2, 5, 3, 6, 2, 2, 4, 3, 3, 4, 2, 2, 1],
    [6, 2, 3, 1, 5, 4, 305, 2, 7, 3, 3, 3, 2, 3, 6, 2, 3, 5, 5, 4, 3, 2, 3, 1],
    [9, 1, 3, 3, 3, 1, 5, 279, 5, 5, 3, 3, 5, 4, 5, 1, 3, 3, 4, 2, 3, 2, 1, 1],
    [6, 4, 2, 4, 5, 4, 4, 3, 280, 4, 4, 4, 5, 4, 5, 2, 5, 5, 4, 5, 3, 3, 3, 2],
    [7, 3, 3, 5, 2, 5, 4, 4, 5, 276, 4, 5, 5, 4, 5, 1, 3, 3, 5, 4, 5, 4, 2, 2],
    [8, 3, 1, 2, 2, 4, 3, 1, 4, 5, 382, 3, 4, 2, 5, 2, 3, 2, 2, 3, 2, 3, 1, 1],
    [11, 2, 2, 3, 2, 3, 2, 2, 4, 4, 2, 233, 3, 3, 5, 2, 4, 4, 5, 3, 3, 3, 2, 2],
    [9, 4, 3, 4, 2, 4, 5, 2, 4, 4, 3, 2, 343, 4, 6, 2, 4, 5, 4, 5, 4, 4, 4, 2],
    [6, 2, 1, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 258, 5, 2, 4, 3, 3, 3, 3, 2, 2, 1],
    [8, 4, 3, 4, 5, 4, 5, 4, 5, 5, 3, 3, 4, 5, 374, 4, 5, 5, 4, 5, 3, 3, 2, 1],
    [4, 2, 3, 4, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 4, 425, 3, 4, 3, 4, 3, 3, 2, 1],
    [5, 2, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 4, 3, 5, 4, 368, 3, 3, 4, 3, 3, 2, 1],
    [5, 2, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 394, 4, 5, 5, 5, 4, 2],
    [7, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 395, 4, 3, 3, 3, 1],
    [8, 4, 3, 4, 5, 3, 4, 3, 5, 3, 4, 3, 5, 3, 4, 5, 5, 5, 4, 326, 4, 3, 3, 2],
    [7, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 3, 3, 3, 3, 278, 3, 3, 1],
    [5, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 435, 3, 1],
    [6, 4, 3, 4, 4, 4, 4, 3, 5, 3, 3, 4, 5, 4, 4, 3, 3, 4, 4, 4, 4, 4, 343, 2],
    [4, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 2, 2, 3, 3, 3, 3, 3, 3, 402],
])

# Define the labels
labels = ["bed", "down", "eight", "five", "four", "go", "happy", "house", "left", "nine", 
          "no", "off", "on", "one", "right", "seven", "six", "stop", "three", "two", 
          "up", "wow", "yes", "zero"]

# Calculate evaluation metrics
def calculate_metrics(confusion_matrix):
    true_labels = np.repeat(np.arange(confusion_matrix.shape[0]), confusion_matrix.sum(axis=1))
    pred_labels = np.hstack([np.repeat(i, confusion_matrix[:, i].sum()) for i in range(confusion_matrix.shape[1])])
    
    accuracy = accuracy_score(true_labels, pred_labels)
    precision = precision_score(true_labels, pred_labels, average='weighted')
    recall = recall_score(true_labels, pred_labels, average='weighted')
    f1 = f1_score(true_labels, pred_labels, average='weighted')
    
    return accuracy, precision, recall, f1, true_labels, pred_labels

accuracy, precision, recall, f1, true_labels, pred_labels = calculate_metrics(confusion_matrix)

# Display the confusion matrix with proper labels
plt.figure(figsize=(18, 18))
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)
disp.plot(cmap=plt.cm.Blues, ax=plt.gca())

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')

# Adjust axis labels for better readability
plt.xticks(ticks=np.arange(confusion_matrix.shape[0]), labels=labels, fontsize=10, rotation=90)
plt.yticks(ticks=np.arange(confusion_matrix.shape[0]), labels=labels, fontsize=10)

plt.tight_layout()  # Ensure the layout is neat
plt.show()

# Display evaluation metrics
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Display metrics for each label
report = classification_report(true_labels, pred_labels, target_names=labels)
print(report)
